{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add57f9e-0d5f-4c20-9f0e-6dd902e5830e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().system('pip install fitz')\n",
    "get_ipython().system('pip install pymupdf')\n",
    "get_ipython().system('pip install torch')\n",
    "get_ipython().system('pip install git+https://github.com/huggingface/transformers accelerate')\n",
    "get_ipython().system('pip install qwen-vl-utils')\n",
    "get_ipython().system('pip install langdetect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6249835a-5f33-4f42-b6cb-84584e7735f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import requests\n",
    "import fitz  # PyMuPDF\n",
    "from langdetect import detect\n",
    "import re\n",
    "import pandas as pd\n",
    "import time  # ‚è∞ Import the time module\n",
    "\n",
    "# Track total execution time\n",
    "overall_start = time.time()\n",
    "\n",
    "# Step 1: Set up the model\n",
    "model_start = time.time()\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float16, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU found, using CPU.\")\n",
    "    \n",
    "print(\"‚úÖ Model loaded successfully.\")\n",
    "\n",
    "# Step 2: Download the PDF from URL\n",
    "pdf_url = \"https://arxiv.org/pdf/2211.02001.pdf\"\n",
    "pdf_path = \"sample_paper.pdf\"\n",
    "response = requests.get(pdf_url)\n",
    "with open(pdf_path, 'wb') as f:\n",
    "    f.write(response.content)\n",
    "print(\"‚úÖ PDF downloaded successfully.\")\n",
    "\n",
    "# Step 3: Extract text from multiple pages and split into chunks\n",
    "def extract_text_from_pdf(pdf_path, max_pages=1, chunk_size=800):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text_chunks = []\n",
    "    text = \"\"\n",
    "    for i, page in enumerate(doc):\n",
    "        if i >= max_pages:\n",
    "            break\n",
    "        text += page.get_text()\n",
    "        # Split into chunks with context overlap for coherence\n",
    "        while len(text) > chunk_size:\n",
    "            text_chunks.append(text[:chunk_size + 300])  # Overlap for context\n",
    "            text = text[chunk_size:]\n",
    "    text_chunks.append(text)\n",
    "    return text_chunks\n",
    "\n",
    "text_chunks = extract_text_from_pdf(pdf_path)\n",
    "print(\"‚úÖ Text extracted and split into segments.\")\n",
    "print(\"Number of segments:\", len(text_chunks))\n",
    "\n",
    "# Step 4: Define the question\n",
    "question = \"How much CO2 is emitted during the training of language models according to the article?\"\n",
    "\n",
    "# Step 5: Iterate over text chunks and generate answers\n",
    "all_responses = []\n",
    "\n",
    "for i, chunk in enumerate(text_chunks):\n",
    "    print(f\"\\nüîç Analyzing Segment {i+1}/{len(text_chunks)}...\")\n",
    "\n",
    "    # Improved prompt for conciseness and focus on numbers\n",
    "    input_text = (\n",
    "    f\"Context: {chunk}\\n\"\n",
    "    f\"Question: {question}\\n\"\n",
    "    \"Please provide a short, precise answer focusing on numerical data only. \"\n",
    "    \"If no relevant information is found, reply with 'No data available'.\\n\"\n",
    ")\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    # Step 6: Generate an answer for the current chunk\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=32,  # Shorter answers for conciseness\n",
    "            num_beams=5,    # Beam search for better quality\n",
    "            temperature=0.3,\n",
    "            no_repeat_ngram_size=2,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    # Decode the response and add to list\n",
    "    response = tokenizer.decode(generated_ids[0], skip_special_tokens=True).strip()\n",
    "    all_responses.append(response)\n",
    "    \n",
    "def extract_key_sentences(responses):\n",
    "    key_sentences = []\n",
    "    for response in responses:\n",
    "        if detect(response) == 'en':\n",
    "            sentences = response.split('. ')\n",
    "            for sentence in sentences:\n",
    "                if re.search(r'\\b\\d+(\\.\\d+)?\\b', sentence):\n",
    "                    if any(keyword in sentence.lower() for keyword in [\"co2\", \"carbon dioxide\", \"emission\", \"tonnes\", \"kg\",\"co2eq\",\"carbon footprint\",\"bloom\",\"training\"]):\n",
    "                        sentence = re.sub(r'(Question|Output Format|Please provide).*?\\. ', '', sentence, flags=re.IGNORECASE)\n",
    "                        sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
    "                        key_sentences.append(sentence)\n",
    "    return key_sentences\n",
    "\n",
    "filtered_responses = extract_key_sentences(all_responses)\n",
    "\n",
    "# Step 7: Summarize the answers\n",
    "\n",
    "def summarize_responses(responses):\n",
    "    summary_text = \" \".join(responses)\n",
    "    summary_prompt = (\n",
    "        f\"Summarize the following text in about 100 words, focusing on CO2 emissions and numerical data:\\n\"\n",
    "        f\"{summary_text}\\n\"\n",
    "    )\n",
    "    summary_inputs = tokenizer(summary_prompt, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        summary_ids = model.generate(\n",
    "            **summary_inputs,\n",
    "            max_new_tokens=150,\n",
    "            num_beams=5,\n",
    "            temperature=0.3,\n",
    "            no_repeat_ngram_size=2,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True).strip()\n",
    "    summary = re.sub(r'(Question|Output Format|Please provide).*?\\. ', '', summary, flags=re.IGNORECASE)\n",
    "    summary = re.sub(r'\\s+', ' ', summary).strip()\n",
    "    return summary\n",
    "\n",
    "summary = summarize_responses(filtered_responses)\n",
    "\n",
    "print(\"\\n‚úÖ 100-Word Summary of Main Facts:\")\n",
    "print(summary)\n",
    "\n",
    "# Step 8: Display Top 10 Answers\n",
    "\n",
    "def get_top_10_responses(responses):\n",
    "    unique_responses = list(dict.fromkeys(responses))  # Remove duplicates\n",
    "    sorted_responses = sorted(unique_responses, key=lambda x: len(re.findall(r'\\b\\d+(\\.\\d+)?\\b', x)), reverse=True)\n",
    "    return sorted_responses[:10]\n",
    "\n",
    "top_10_answers = get_top_10_responses(filtered_responses)\n",
    "print(\"\\n‚úÖ Clean Top 10 Relevant Sentences:\")\n",
    "for sentence in top_10_answers:\n",
    "    print(\"-\", sentence)\n",
    "\n",
    "# End timing the entire script\n",
    "overall_end = time.time()\n",
    "\n",
    "# Calculate and display total execution time\n",
    "total_duration = overall_end - overall_start\n",
    "print(f\"\\n‚è±Ô∏è Total execution time: {total_duration / 60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33450300-f018-45fb-8b8d-f32ccf427e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of model names to look for (modify this as needed)\n",
    "model_names = [\n",
    "    \"BLOOM\",\n",
    "    \"GPT-3\",\n",
    "    \"T5\",\n",
    "    \"BERT\",\n",
    "    \"XLNet\",\n",
    "    \"RoBERTa\",\n",
    "    \"GPT-2\",\n",
    "    \"Transformer-XL\",\n",
    "    \"Albert\",\n",
    "    \"Megatron\"\n",
    "]\n",
    "\n",
    "# Enhanced function to extract model names and ALL CO2 emissions from the responses\n",
    "def extract_model_emissions(responses, model_names):\n",
    "    model_emissions = []\n",
    "\n",
    "    # Improved pattern to capture multiple emissions for the same model\n",
    "    pattern = re.compile(\n",
    "        r\"(?P<model>\" + \"|\".join(model_names) + r\").*?(?:approximately|around|about)?\\s?(?P<emission>\\d+(\\.\\d+)?)\\s?(?:tonnes|tonne|t)\\s?(?:CO2|CO2eq|CO‚ÇÇ)?\",\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    for response in responses:\n",
    "        # Split sentences manually using regex\n",
    "        sentences = re.split(r'(?<=[.!?]) +', response)\n",
    "        for sentence in sentences:\n",
    "            # Check if any model name is in the sentence\n",
    "            if any(model in sentence for model in model_names):\n",
    "                print(f\"\\nüîé Analyzing Sentence: {sentence}\")\n",
    "                \n",
    "                # Find all emissions for the same model\n",
    "                matches = pattern.findall(sentence)\n",
    "                if matches:\n",
    "                    for match in matches:\n",
    "                        model = match[0]  # Model name\n",
    "                        emission = float(match[1])  # CO2 emission value\n",
    "                        model_emissions.append({\n",
    "                            \"Model Name\": model,\n",
    "                            \"CO2 Emission\": emission,\n",
    "                            \"Context\": sentence\n",
    "                        })\n",
    "                        print(f\"‚úÖ Model Found: {model}\")\n",
    "                        print(f\"üìè Emission Found: {emission} for Model: {model}\")\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è Model mentioned but no emissions found.\")\n",
    "    \n",
    "    return model_emissions\n",
    "\n",
    "# Extract model emissions from the filtered responses\n",
    "model_emissions_list = extract_model_emissions(filtered_responses, model_names)\n",
    "\n",
    "# Create DataFrame from the extracted data\n",
    "df_emissions = pd.DataFrame(model_emissions_list)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"\\nüìä CO2 Emissions by Model with Context:\")\n",
    "print(df_emissions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
